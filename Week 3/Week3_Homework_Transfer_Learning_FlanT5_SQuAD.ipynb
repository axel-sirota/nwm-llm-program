{"cells":[{"cell_type":"markdown","id":"217fb889","metadata":{"id":"217fb889"},"source":["# Transfer Learning with Flan-T5 on SQuAD Dataset\n","\n","**Author**: Axel Sirota\n","\n","In this notebook, we will explore the process of transfer learning using the `flan-t5-base` model (or large if you have sufficient hardware), fine-tuning it on the SQuAD (Stanford Question Answering Dataset) for a question-answering task. We'll focus on preparing the data, fine-tuning the model, and evaluating its performance."]},{"cell_type":"markdown","id":"553adcd5","metadata":{"id":"553adcd5"},"source":["### Introduction to SQuAD and Flan-T5-base\n","\n","The SQuAD dataset is a large-scale dataset for question answering. It contains a series of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text, or span, from the corresponding reading passage.\n","\n","`flan-t5-base` is a variant of the T5 (Text-To-Text Transfer Transformer) model, specifically fine-tuned on a wide variety of datasets to generalize well to different tasks. In this notebook, we will use this model for a question-answering task on the SQuAD dataset."]},{"cell_type":"markdown","id":"d84f9f55","metadata":{"id":"d84f9f55"},"source":["## Preparation\n","\n","First, let's ensure we have all necessary libraries installed. We'll be using the `transformers` library for the model and tokenizer, as well as the `datasets` library to load and process the SQuAD dataset, and TensorFlow for training."]},{"cell_type":"code","execution_count":null,"id":"cf60310f","metadata":{"id":"cf60310f"},"outputs":[],"source":["# Install required libraries\n","!pip install -U transformers datasets tensorflow"]},{"cell_type":"markdown","id":"53c32375","metadata":{"id":"53c32375"},"source":["### Importing Libraries\n","\n","After installing the required packages, let's import them. We'll also set up some global parameters such as `EPOCHS` and `BATCH_SIZE` for fine-tuning."]},{"cell_type":"code","execution_count":null,"id":"3c9ca8e0","metadata":{"id":"3c9ca8e0"},"outputs":[],"source":["# Import necessary libraries\n","import tensorflow as tf  # TensorFlow for deep learning\n","from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n","from datasets import load_dataset\n","import numpy as np\n","import warnings\n","\n","# Set up global parameters for fine-tuning\n","EPOCHS = 3  # Number of training epochs\n","BATCH_SIZE = 8  # Batch size for training (adjust based on your hardware)\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","id":"b8404b78","metadata":{"id":"b8404b78"},"source":["## Loading and Preparing the SQuAD Dataset\n","\n","We'll load the SQuAD dataset using the `datasets` library. This dataset contains a collection of articles from Wikipedia, each paired with a set of questions and their corresponding answers. We'll preprocess the dataset and convert it into a format suitable for TensorFlow."]},{"cell_type":"code","execution_count":null,"id":"9c005da5","metadata":{"id":"9c005da5"},"outputs":[],"source":["# Load the SQuAD dataset\n","dataset = None\n","\n","# Display a sample from the training set\n","print(dataset['train'][0])"]},{"cell_type":"markdown","id":"322e2cbe","metadata":{"id":"322e2cbe"},"source":[":### Tokenization and Data Preparation\n","\n","We need to tokenize the input data and prepare it for the `flan-t5-base` model. We'll use the `AutoTokenizer` from the `transformers` library to handle this step. The tokenizer will convert text into tokens that the model can understand, and we'll create TensorFlow datasets for training and validation.\n","\n","It is key to remember to truncate and pad since T5 requires all inputs to have the same shape. Also in seq2seq models apart from the `input_ids` we need to generate the `decoder_input_ids` which would play the role of the input to the decoder"]},{"cell_type":"code","execution_count":null,"id":"c0b0442d","metadata":{"id":"c0b0442d"},"outputs":[],"source":["# Load the tokenizer for flan-t5-base\n","tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n","\n","def preprocess_function(examples):\n","    # Combine the context and the question for the input to the model as a list of strings\n","    inputs = None\n","\n","    # Tokenize the inputs (questions + contexts) with max_length 512 and remember to truncate and pad\n","    model_inputs = None\n","\n","    # Tokenize the targets (answers) to max_length 128\n","    labels = None\n","\n","    # The labels are the decoder's input ids\n","    model_inputs['labels'] = labels['input_ids']\n","    model_inputs['decoder_input_ids'] = labels['input_ids']\n","\n","    return model_inputs\n","\n","# Apply the preprocessing to the training and validation sets\n","train_tokenized_datasets = dataset['train'].select(range(10000)).map(preprocess_function, batched=True)   # We just use 10000 examples to make training go faster\n","validation_tokenized_datasets = dataset['validation'].select(range(1000)).map(preprocess_function, batched=True)  # We just use 1000 examples to make training go faster"]},{"cell_type":"code","source":["\n","# Convert datasets to TensorFlow format\n","train_dataset = None\n","\n","val_dataset = None"],"metadata":{"id":"hSOeipyCkMO-"},"id":"hSOeipyCkMO-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for input, labels in train_dataset.take(1):\n","    print(input['input_ids'].shape)\n","    print(input['decoder_input_ids'].shape)\n","    print(labels.shape)"],"metadata":{"id":"foXpkkBVqxG2"},"id":"foXpkkBVqxG2","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"cbadf71f","metadata":{"id":"cbadf71f"},"source":["## Fine-Tuning the Flan-T5-Large Model\n","\n","With the data prepared, we can now fine-tune the `flan-t5-large` model on the SQuAD dataset using TensorFlow."]},{"cell_type":"code","execution_count":null,"id":"16eeced4","metadata":{"id":"16eeced4"},"outputs":[],"source":["# Load the pre-trained flan-t5-base model\n","model = None\n","\n","# Compile the model\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n","    loss=model.hf_compute_loss,  # Use the model's built-in loss function\n",")\n","\n","# Print the model summary\n","model.summary()"]},{"cell_type":"code","source":["# Make the encoder, embedding and decoder layers non trainable\n","\n","model.summary()"],"metadata":{"id":"JPI9C4u4k3yx"},"id":"JPI9C4u4k3yx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the model"],"metadata":{"id":"e1CIecuIlGko"},"id":"e1CIecuIlGko","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"fb320a7c","metadata":{"id":"fb320a7c"},"source":["## Evaluating the Model\n","\n","After fine-tuning, we will evaluate the model's performance on the validation set. We will use the predictions to calculate the Exact Match (EM) and F1 scores, which are common metrics for question answering tasks."]},{"cell_type":"code","execution_count":null,"id":"dd4baad2","metadata":{"id":"dd4baad2"},"outputs":[],"source":["# Evaluate the model on the validation set\n","results = model.evaluate(val_dataset)\n","\n","# Display the evaluation metrics\n","print('Validation Loss:', results)"]},{"cell_type":"markdown","source":["**Homework**: Think of a nice way to manually test this model!"],"metadata":{"id":"7Ballxb4sgHx"},"id":"7Ballxb4sgHx"},{"cell_type":"code","source":[],"metadata":{"id":"Db-CfMUbskeb"},"id":"Db-CfMUbskeb","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"language_info":{"name":"python"},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}