{"cells":[{"cell_type":"markdown","id":"983070ba","metadata":{"id":"983070ba"},"source":["# Transfer Learning with Transformers using Hugging Face\n","\n","© Data Trainers LLC. GPL v 3.0.\n","\n","**Author**: Axel Sirota\n","\n","In this notebook, we will explore the process of transfer learning using transformers, specifically leveraging the Hugging Face library. Hugging Face offers a range of pre-trained models that allow you to quickly implement state-of-the-art NLP solutions without the need to train models from scratch. We will guide you through the steps to prepare, fine-tune, and evaluate a transformer model for a specific NLP task."]},{"cell_type":"markdown","id":"9e7b884a","metadata":{"id":"9e7b884a"},"source":["### Introduction to Hugging Face\n","\n","Hugging Face is an open-source company that provides a wide array of transformer models and tools for NLP tasks. With their easy-to-use API and extensive model repository, you can implement advanced machine learning models with minimal effort. In this notebook, we'll focus on using these resources to apply transfer learning, which involves taking a pre-trained model and fine-tuning it on a new dataset to solve a different task."]},{"cell_type":"markdown","id":"a7b10b44","metadata":{"id":"a7b10b44"},"source":["## Preparation\n","\n","Before we dive into the code, let's make sure we have all the necessary libraries installed. We'll be using the `datasets`, `evaluate`, and `transformers` libraries from Hugging Face, along with `sentencepiece` for handling various NLP tasks."]},{"cell_type":"code","execution_count":null,"id":"ece67169","metadata":{"id":"ece67169"},"outputs":[],"source":["# Install required libraries\n","!pip install -U datasets evaluate transformers transformers[sentencepiece]"]},{"cell_type":"markdown","id":"9d383173","metadata":{"id":"9d383173"},"source":["### Importing Libraries\n","\n","Now that we have the required packages installed, let's import them into our environment. We will also set some global parameters such as `EPOCHS` and `BATCH_SIZE`, and suppress any unnecessary warnings."]},{"cell_type":"code","execution_count":null,"id":"48bf00e5","metadata":{"id":"48bf00e5"},"outputs":[],"source":["# Import necessary libraries for the task\n","import tensorflow as tf  # TensorFlow for deep learning\n","from datasets import load_dataset  # Hugging Face's library to load datasets\n","from transformers import AutoTokenizer  # Tokenizer from Hugging Face's transformers library\n","import numpy as np  # NumPy for numerical operations\n","import random  # Random module for reproducibility\n","import pandas as pd  # Pandas for data manipulation\n","import warnings  # Warnings to control unnecessary alerts\n","\n","# Define global parameters for training\n","EPOCHS = 3  # Number of training epochs\n","BATCH_SIZE = 64  # Batch size for training\n","# Suppress warnings for a cleaner output\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","id":"a18c8b76","metadata":{"id":"a18c8b76"},"source":["## Tokenizing and loading the dataset\n","\n","In HuggingFace there are many models, and each has its own tokenizer. Lucky for us there is a class `AutoTokenizer` that does the heavylifting after we provide a checkpoint"]},{"cell_type":"code","execution_count":null,"id":"e2943e6b","metadata":{"id":"e2943e6b"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","import numpy as np\n","\n","raw_datasets = load_dataset(\"imdb\")  # load imdb dataset\n","raw_datasets"]},{"cell_type":"markdown","id":"fa05e3a6","metadata":{"id":"fa05e3a6"},"source":["Notice it is a dict object with the train, test, and unsupervised datasets to play around"]},{"cell_type":"code","execution_count":null,"id":"87c5a881","metadata":{"id":"87c5a881"},"outputs":[],"source":["raw_datasets['train'][0]  # Let's see the first review"]},{"cell_type":"markdown","id":"24ba6816","metadata":{"id":"24ba6816"},"source":["How do we know if it's positive or negative from label=0?"]},{"cell_type":"code","execution_count":null,"id":"a64fee98","metadata":{"id":"a64fee98"},"outputs":[],"source":["raw_datasets['train'].features"]},{"cell_type":"markdown","id":"90a50504","metadata":{"id":"90a50504"},"source":["There it is, within features we see that the index 0 is **Negative**\n","\n","Now to tokenise the dataset we need to load the proper tokenizer for the model we care about. And the we are goin to apply it everywhere!\n","\n","After this step the tokenizer converts the text into a Tensor of ids, each representing a diferent word in the BERT vocabulary"]},{"cell_type":"code","execution_count":null,"id":"82d04926","metadata":{"id":"82d04926"},"outputs":[],"source":["checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = None # Fetch the tokenizer for that checkpoint\n","\n","\n","def tokenize_function(example):\n","    # We are using the BERT tokenizer, specifying to PAD until the end,\n","    # truncate if either 128 elements are met or the maximum from the model, which you get from the model card\n","\n","    return pass # Return a tokenizer function that adds padding to 128 chars and truncates from the examples\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n"]},{"cell_type":"markdown","id":"c8195465","metadata":{"id":"c8195465"},"source":["Let's see how it worked!"]},{"cell_type":"code","execution_count":null,"id":"d5abf81e","metadata":{"id":"d5abf81e"},"outputs":[],"source":["tokenized_datasets['train'][0]['text']"]},{"cell_type":"code","execution_count":null,"id":"2ade2abf","metadata":{"id":"2ade2abf"},"outputs":[],"source":["tokenizer(tokenized_datasets['train'][0]['text'])"]},{"cell_type":"markdown","id":"17ed6bb0","metadata":{"id":"17ed6bb0"},"source":["The tokenizer from BERT (well DistillBERT) converts each word into its ID according to *its* vocabulary. And notice the masking says we haven't been truncated. What we will do know is do this for all data and convert it into a TF Datasets object (which Keras accepts)"]},{"cell_type":"code","execution_count":null,"id":"bfbacef5","metadata":{"id":"bfbacef5"},"outputs":[],"source":["\n","tf_train_dataset = None # Convert the tokenized_datasets[\"train\"] to a TF Dataset\n","\n","\n","tf_validation_dataset = None. # Same with validation"]},{"cell_type":"code","execution_count":null,"id":"12b6a870","metadata":{"id":"12b6a870"},"outputs":[],"source":["for inputs, labels in tf_train_dataset.take(1):\n","  print(f' inputs: {inputs.shape}, labels: {labels.shape}')\n"]},{"cell_type":"markdown","id":"273ea20c","metadata":{"id":"273ea20c"},"source":["## Downloading the model and prepare for training"]},{"cell_type":"markdown","id":"2a8bc579","metadata":{"id":"2a8bc579"},"source":["Now let's download the model. It is very important you use the class that starts with `TFAutoModel`. There are auto models for most tasks, so you don't have to manually add the header, for example the `TFAutoModelForSequenceClassification` adds a Dense layer (WITHOUT SOFTMAX) to do the classification"]},{"cell_type":"code","execution_count":null,"id":"0e60e68b","metadata":{"id":"0e60e68b"},"outputs":[],"source":["\n","model = None # Download the model for sequence classification with 2 labels (sentiment analysis)"]},{"cell_type":"code","execution_count":null,"id":"55d52480","metadata":{"id":"55d52480"},"outputs":[],"source":["loss = None  # Set the loss\n","# Compile the model"]},{"cell_type":"code","execution_count":null,"id":"d08bfa4a","metadata":{"id":"d08bfa4a"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","id":"6712e076","metadata":{"id":"6712e076"},"source":["Oh no! We have too many parameters to train! Luckily in Keras is very easy to set some layers as not trainable"]},{"cell_type":"code","execution_count":null,"id":"1ec1e381","metadata":{"id":"1ec1e381"},"outputs":[],"source":["# Set the first layer as non trainable"]},{"cell_type":"code","execution_count":null,"id":"345556e2","metadata":{"id":"345556e2"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","id":"c4487457","metadata":{"id":"c4487457"},"source":["*Voilá!*"]},{"cell_type":"code","execution_count":null,"id":"e5a313fc","metadata":{"id":"e5a313fc"},"outputs":[],"source":["# Fit the model"]},{"cell_type":"markdown","id":"fb666f67","metadata":{"id":"fb666f67"},"source":["Now we have a trained model that did transfer learning from DistillBERT"]},{"cell_type":"markdown","id":"71e41dd9","metadata":{"id":"71e41dd9"},"source":["## Testing it out!"]},{"cell_type":"code","execution_count":null,"id":"29718d9b","metadata":{"id":"29718d9b"},"outputs":[],"source":["tokens = tokenizer([\"This is the worst internet service provider\", \"Although most people say this is the worst, I like it\"], padding=True, truncation=True, max_length=128)"]},{"cell_type":"code","execution_count":null,"id":"8f442382","metadata":{"id":"8f442382"},"outputs":[],"source":["tokens"]},{"cell_type":"code","execution_count":null,"id":"10182db7","metadata":{"id":"10182db7"},"outputs":[],"source":["model.predict(tokens['input_ids'])"]},{"cell_type":"markdown","id":"f95be42c","metadata":{"id":"f95be42c"},"source":["Notice the prediction where not probabilities but logits!"]},{"cell_type":"code","execution_count":null,"id":"7fb9cae1","metadata":{"id":"7fb9cae1"},"outputs":[],"source":["tf.math.softmax(model.predict(tokens['input_ids'])['logits'])"]},{"cell_type":"code","execution_count":null,"id":"cfe4ba81","metadata":{"id":"cfe4ba81"},"outputs":[],"source":["tf.math.argmax(tf.math.softmax(model.predict(tokens['input_ids'])['logits']))"]},{"cell_type":"markdown","id":"8d94ab1b","metadata":{"id":"8d94ab1b"},"source":["And the model was correct!!"]},{"cell_type":"code","execution_count":null,"id":"2d8ebaf7","metadata":{"id":"2d8ebaf7"},"outputs":[],"source":["model.evaluate(tf_validation_dataset)"]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}