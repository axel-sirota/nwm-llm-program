{"cells":[{"cell_type":"markdown","id":"3e549e5d","metadata":{"id":"3e549e5d"},"source":["\n","# Multimodal Training Using Hugging Face, Keras, and TensorFlow\n","\n","This notebook guides you through the process of training a multimodal model that can handle both text and image inputs. We'll use the Flickr8k dataset, which contains images paired with textual descriptions, and a small language model (LLM) from Hugging Face.\n","\n","## 1. Environment Setup\n","\n","Ensure you have the necessary Python packages installed.\n","    "]},{"cell_type":"code","execution_count":null,"id":"fc77efb0","metadata":{"id":"fc77efb0"},"outputs":[],"source":["\n","!pip install tensorflow transformers datasets tensorflow_hub matplotlib 'keras==3.2'\n"]},{"cell_type":"markdown","id":"feb5ad82","metadata":{"id":"feb5ad82"},"source":["\n","## 2. Load the Hugging Face Model\n","\n","We'll start by loading a small language model (LLM) from Hugging Face that we'll use to process text data. For this example, we'll use the `distilbert-base-uncased` model.\n","    "]},{"cell_type":"code","execution_count":null,"id":"e95ed80f","metadata":{"id":"e95ed80f"},"outputs":[],"source":["\n","from transformers import AutoTokenizer, TFAutoModel\n","import tensorflow as tf\n","\n","BATCH_SIZE = 128  # Adjust based on your hardware and RAM\n","EPOCHS = 5\n","PATIENCE = 5\n","\n","# Set the seed for reproducibility\n","tf.random.set_seed(42)\n","\n","# Load the tokenizer for the DistilBERT model\n","tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# Load the DistilBERT model itself\n","text_model = TFAutoModel.from_pretrained('distilbert-base-uncased')\n"]},{"cell_type":"markdown","id":"9188af9d","metadata":{"id":"9188af9d"},"source":["\n","## 3. Prepare the Image Model\n","\n","Next, we prepare a model for processing images. We use a pre-trained model like `MobileNetV2` from TensorFlow, which is lightweight and effective for this task.\n","    "]},{"cell_type":"code","execution_count":null,"id":"c079f34b","metadata":{"id":"c079f34b"},"outputs":[],"source":["from tensorflow.keras.applications import MobileNetV2\n","\n","# Load a pre-trained MobileNetV2 model, excluding the top layers (which are meant for classification)\n","image_model = MobileNetV2(include_top=False, input_shape=(224, 224, 3))\n","\n","# Add a GlobalAveragePooling2D layer to reduce the spatial dimensions of the feature map\n","image_model = None"]},{"cell_type":"markdown","id":"27bb42f5","metadata":{"id":"27bb42f5"},"source":["\n","## 4. Load and Preprocess the Dataset\n","\n","We will now load the Flickr8k dataset, which includes images and captions. We'll preprocess the images and captions, making them ready for input into our model.\n","    "]},{"cell_type":"code","execution_count":null,"id":"9910c9ba","metadata":{"id":"9910c9ba"},"outputs":[],"source":["\n","from datasets import load_dataset\n","\n","# Load the Flickr8k dataset from the Hugging Face datasets library\n","dataset = load_dataset(\"jxie/flickr8k\", split='train[:500]')  # We use 500 examples to make it fast\n"]},{"cell_type":"code","execution_count":null,"id":"UiSL1CW3_gGC","metadata":{"id":"UiSL1CW3_gGC"},"outputs":[],"source":["dataset[0]"]},{"cell_type":"code","execution_count":null,"id":"1xP6yUp6_dhc","metadata":{"id":"1xP6yUp6_dhc"},"outputs":[],"source":["import random\n","import numpy as np\n","from PIL import Image\n","\n","def preprocess_text(example, label_index):\n","    caption_key = f'caption_{label_index}'\n","    encodings = None  # Apply the tokenizer\n","    return {'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask'], 'original': example}\n","\n","def preprocess_image(example):\n","    try:\n","        image = example['image']  # This is already a PIL image\n","        image = image.convert('RGB')\n","        image = None  # Convert PIL image to array\n","        image = None  # Resize the image to the required input size\n","        image = None  # Normalize the image\n","        return image\n","    except Exception as e:\n","        print(f\"Error processing image: {e}\")\n","        return None  # Return None if the image is bad\n","\n","def map_function(example):\n","    label_index = random.randint(0, 4)\n","    return {\n","        'input_ids': preprocess_text(example, label_index)['input_ids'],\n","        'attention_mask': preprocess_text(example, label_index)['attention_mask'],\n","        'image': preprocess_image(example),\n","        'label': label_index\n","    }"]},{"cell_type":"code","execution_count":null,"id":"lbm8U3snN1u8","metadata":{"id":"lbm8U3snN1u8"},"outputs":[],"source":["dataset = dataset.map(map_function)"]},{"cell_type":"code","execution_count":null,"id":"xwG0v5VtsJra","metadata":{"id":"xwG0v5VtsJra","collapsed":true},"outputs":[],"source":["dataset[0]"]},{"cell_type":"code","execution_count":null,"id":"ZC7ciP_dF1h2","metadata":{"id":"ZC7ciP_dF1h2"},"outputs":[],"source":["import tensorflow as tf\n","\n","# Ensure that the images are in the correct float32 format and text inputs are in int32\n","def prepare_dataset(dataset):\n","    return tf.data.Dataset.from_tensor_slices((\n","        {\n","            'input_ids': None,  # Text input IDs\n","            'attention_mask': None,  # Attention masks\n","            'image_input': None,  # Image inputs\n","        },\n","        None  # Labels\n","    )).shuffle(1000).batch(BATCH_SIZE)\n","\n","train_dataset = prepare_dataset(dataset)"]},{"cell_type":"code","execution_count":null,"id":"1Vzr0f8RF9Vk","metadata":{"id":"1Vzr0f8RF9Vk"},"outputs":[],"source":["for batch in train_dataset.take(1):\n","    print(batch[0]['input_ids'].shape)\n","    print(batch[0]['attention_mask'].shape)\n","    print(batch[0]['image_input'].shape)\n","    print(batch[1].shape)"]},{"cell_type":"markdown","id":"8ebf9407","metadata":{"id":"8ebf9407"},"source":["\n","## 5. Build the Multimodal Model\n","\n","Now that we have models for both text and image data, we can combine these into a single multimodal model. This model will take both text and image inputs and output a prediction.\n","    "]},{"cell_type":"code","execution_count":null,"id":"sHGVEvkzDcBj","metadata":{"id":"sHGVEvkzDcBj"},"outputs":[],"source":["from tensorflow.keras.layers import Layer\n","\n","class ReduceMeanLayer(Layer):\n","    def call(self, inputs):\n","        return tf.reduce_mean(inputs, axis=1)\n","\n","# Instantiate the custom layer\n","reduce_mean_layer = ReduceMeanLayer()\n"]},{"cell_type":"code","execution_count":null,"id":"a50329eb","metadata":{"id":"a50329eb"},"outputs":[],"source":["from tensorflow.keras.layers import Input, Lambda, Dense, Concatenate\n","from tensorflow.keras.models import Model\n","\n","# Define the input layers for text and images. Name them input_ids, attention_mask and image_input\n","input_ids = None\n","attention_mask = None\n","image_input = None\n","\n","# Process the text inputs using a Lambda layer\n","def distilbert_encode(inputs):\n","    input_ids = inputs['input_ids']\n","    attention_mask = inputs['attention_mask']\n","    return text_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n","\n","text_features = Lambda(distilbert_encode, output_shape=(128, 768))({\n","    'input_ids': input_ids,\n","    'attention_mask': attention_mask\n","})\n","\n","# Apply the custom reduce mean layer to the text features\n","reduced_text_features = None\n","\n","# Process the image input through the image model\n","image_features = None\n","\n","# Combine (concatenate) the features from both modalities\n","combined_features = Concatenate()([reduced_text_features, image_features])\n","\n","# Add a dense layer for learning complex patterns\n","dense_layer = Dense(128, activation='relu')(combined_features)\n","\n","# Output layer with 5 units (for 5 possible captions), using softmax activation\n","output = None\n","\n","# Create the full multimodal model\n","multimodal_model = Model(inputs=[input_ids, attention_mask, image_input], outputs=output)\n"]},{"cell_type":"code","execution_count":null,"id":"N_Gy6ijhCPjV","metadata":{"id":"N_Gy6ijhCPjV"},"outputs":[],"source":["multimodal_model.summary()"]},{"cell_type":"markdown","id":"56b1d37b","metadata":{"id":"56b1d37b"},"source":["\n","## 6. Compile and Train the Model\n","\n","We now compile the model using an appropriate loss function and optimizer. After that, we'll train the model on our dataset.\n","    "]},{"cell_type":"code","execution_count":null,"id":"twOAJTFJFSBA","metadata":{"id":"twOAJTFJFSBA"},"outputs":[],"source":["# Compile the model\n","multimodal_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model for 5 epochs\n","history = None\n"]},{"cell_type":"markdown","id":"b61f6de9","metadata":{"id":"b61f6de9"},"source":["\n","## 7. Evaluate the Model\n","\n","After training, we evaluate the model to see how well it performs on the training data. We also plot the training history to visualize the accuracy and loss.\n","    "]},{"cell_type":"code","execution_count":null,"id":"73e7abe4","metadata":{"id":"73e7abe4"},"outputs":[],"source":["\n","import matplotlib.pyplot as plt\n","\n","# Evaluate the model to check its performance\n","results = multimodal_model.evaluate(train_dataset)\n","print(f'Test loss: {results[0]}, Test accuracy: {results[1]}')\n","\n","# Plot the training history for accuracy\n","plt.plot(history.history['accuracy'], label='accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"XrjuI3jPJ9ek","metadata":{"id":"XrjuI3jPJ9ek"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}