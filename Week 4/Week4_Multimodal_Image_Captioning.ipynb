{"cells":[{"cell_type":"markdown","id":"086bd042","metadata":{"id":"086bd042"},"source":["# Multimodal Image Captioning using ResNet50 and FLAN-T5"]},{"cell_type":"markdown","id":"4dd63f77","metadata":{"id":"4dd63f77"},"source":["\n","This notebook demonstrates how to create a multimodal model that combines image features extracted from ResNet50 with text generation using the FLAN-T5 model. The model is trained to generate captions for images using the Flickr8k dataset.\n"]},{"cell_type":"code","execution_count":null,"id":"uGXYrxy3vD17","metadata":{"id":"uGXYrxy3vD17"},"outputs":[],"source":["# Install necessary libraries for the project\n","!pip install 'keras==3.2' datasets tensorflow"]},{"cell_type":"markdown","id":"1d2e4821","metadata":{"id":"1d2e4821"},"source":["## Step 1: Load and Preprocess the Data\n","In this step, we load the data from the Flickr8k dataset. We'll preprocess it to feed both image and text data into our model."]},{"cell_type":"code","execution_count":null,"id":"d3db3782","metadata":{"id":"d3db3782"},"outputs":[],"source":["# Import necessary libraries for data loading and processing\n","# The flickr8k dataset is being loaded from the Hugging Face datasets library\n","# We're only using a small subset of 10 images for demonstration purposes\n","\n","from datasets import load_dataset\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","\n","# Load the flickr8k dataset\n","dataset = load_dataset(\"Naveengo/flickr8k\", split='train[:10]')\n"]},{"cell_type":"markdown","id":"51d16348","metadata":{"id":"51d16348"},"source":["## Step 2: Define the Model Architecture\n","Here, we define the multimodal model architecture. We will use a pre-trained ResNet50 model for image features and a FLAN-T5 model for generating text captions."]},{"cell_type":"code","execution_count":null,"id":"c1557e96","metadata":{"id":"c1557e96"},"outputs":[],"source":["# Define the architecture of the multimodal model\n","# We start with a pretrained ResNet50 model for image feature extraction\n","# The image input shape is set to (224, 224, 3), which is common for pre-trained models\n","# The top layers are removed to allow for custom layers, and we freeze the layers to avoid training them\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Input, Dense, Concatenate, Lambda\n","from tensorflow.keras.models import Model\n","from transformers import TFAutoModelForSeq2SeqLM\n","import tensorflow as tf\n","\n","# Load ResNet50 without the top layers (for feature extraction)\n","image_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n","image_model.trainable = False  # Freeze the layers\n","\n","# Define image input\n","image_input = Input(shape=(224, 224, 3), name='image_input')\n","image_features = image_model(image_input)\n","image_features = GlobalAveragePooling2D()(image_features)\n","\n","# Load the FLAN-T5 model\n","text_model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n","\n","# Define text inputs\n","input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')\n","attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n","\n","# Wrap the encoder in a Lambda layer with output shape specified\n","encoder_outputs = Lambda(\n","    lambda inputs: text_model.encoder(input_ids=inputs[0], attention_mask=inputs[1]).last_hidden_state,\n","    output_shape=(128, 768)\n",")([input_ids, attention_mask])\n","\n","\n","# Expand image features to match the sequence length of the text (128)\n","image_features_expanded = Lambda(lambda x: tf.expand_dims(x, 1), output_shape=(1, 2048))(image_features)\n","image_features_expanded = Lambda(lambda x: tf.tile(x, [1, 128, 1]), output_shape=(128, 2048))(image_features_expanded)\n","image_features_expanded = Dense(768)(image_features_expanded)\n","\n","# Combine image features with text encoder outputs\n","combined_features = Concatenate(axis=-1)([encoder_outputs, image_features_expanded])  # Resulting shape should be (128, 1536)\n","\n","projected_features = Dense(768)(combined_features)  # Shape: (None, 128, 768)\n","\n","\n","# Define the decoder inputs\n","decoder_input_ids = Input(shape=(128,), dtype=tf.int32, name='decoder_input_ids')\n","decoder_attention_mask = Input(shape=(128,), dtype=tf.int32, name='decoder_attention_mask')\n","\n","decoder_outputs = Lambda(\n","    lambda inputs: text_model.decoder(input_ids=inputs[0], attention_mask=inputs[1], encoder_hidden_states=inputs[2]).last_hidden_state\n",")([decoder_input_ids, decoder_attention_mask, projected_features])\n","\n","\n","# The decoder outputs are usually a tuple, so we get the first element\n","decoder_last_hidden_state = decoder_outputs.last_hidden_state if isinstance(decoder_outputs, tuple) else decoder_outputs\n","\n","# Final dense layer to generate logits for the vocabulary\n","output = Lambda(lambda x: text_model.lm_head(x), output_shape=(128, text_model.config.vocab_size))(decoder_last_hidden_state)\n","\n","# Define the complete model\n","multimodal_model = Model(\n","    inputs=[image_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask],\n","    outputs=output\n",")\n","\n","# Compile the model\n","multimodal_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"]},{"cell_type":"code","execution_count":null,"id":"FkTVr13uyCMO","metadata":{"id":"FkTVr13uyCMO"},"outputs":[],"source":["multimodal_model.summary()"]},{"cell_type":"markdown","id":"81411880","metadata":{"id":"81411880"},"source":["## Step 3: Prepare the Dataset for Training\n","In this step, we prepare our dataset, converting images and text into tensor format to be compatible with TensorFlow's training process."]},{"cell_type":"code","execution_count":null,"id":"ae40b953","metadata":{"id":"ae40b953"},"outputs":[],"source":["# Define the architecture of the multimodal model\n","# We start with a pretrained ResNet50 model for image feature extraction\n","# The image input shape is set to (224, 224, 3), which is common for pre-trained models\n","# The top layers are removed to allow for custom layers, and we freeze the layers to avoid training them\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","\n","# Preprocess the images for ResNet50\n","def preprocess_image(image):\n","    image = tf.image.resize(image, (224, 224))\n","    image = tf.keras.applications.resnet50.preprocess_input(image)  # This scales pixels to [-1, 1]\n","    return image\n","\n","# Preprocess the captions for FLAN-T5\n","tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n","\n","def preprocess_caption(caption):\n","    encoding = tokenizer(caption, padding='max_length', truncation=True, max_length=128, return_tensors=\"tf\")\n","    return encoding['input_ids'], encoding['attention_mask']\n","\n","def preprocess_function(batch):\n","    # Preprocess images in the batch\n","    images = [preprocess_image(tf.keras.preprocessing.image.img_to_array(item)) for item in batch['image']]\n","    images = tf.stack(images)  # Stack into a single tensor\n","\n","    # Preprocess captions in the batch\n","    input_ids, attention_mask = preprocess_caption(batch['text'])\n","\n","    # Create decoder input ids (shifted input_ids) with the same length\n","    decoder_input_ids = tf.concat([tf.fill([tf.shape(input_ids)[0], 1], tokenizer.pad_token_id), input_ids[:, :-1]], axis=1)\n","\n","    # Ensure that all tensors are the correct shape (i.e., 128 tokens)\n","    input_ids = tf.ensure_shape(input_ids, [None, 128])\n","    attention_mask = tf.ensure_shape(attention_mask, [None, 128])\n","    decoder_input_ids = tf.ensure_shape(decoder_input_ids, [None, 128])\n","\n","    # Return a dictionary with tensors\n","    return {\n","        'input_ids': input_ids,  # Tensor of input_ids\n","        'attention_mask': attention_mask,  # Tensor of attention_mask\n","        'decoder_input_ids': decoder_input_ids,  # Shifted input_ids for decoder\n","        'decoder_attention_mask': attention_mask,  # Usually same as attention_mask for decoder\n","        'image_input': images  # Tensor of processed images\n","    }\n","\n","# Apply the preprocessing function\n","processed_dataset = dataset.map(preprocess_function, batched=True)\n"]},{"cell_type":"code","execution_count":null,"id":"_sjJp7MJCPW7","metadata":{"id":"_sjJp7MJCPW7"},"outputs":[],"source":["# Prepare the dataset for training using TensorFlow\n","# TensorFlow datasets are used for efficient data handling and batching during training\n","# Convert necessary components of the dataset (image inputs and text inputs) into tensors\n","def prepare_dataset(dataset):\n","    input_ids_tensor = tf.convert_to_tensor(dataset['input_ids'])  # Convert list to TensorFlow tensor\n","    attention_mask_tensor = tf.convert_to_tensor(dataset['attention_mask'])  # Convert attention mask as well\n","    decoder_input_ids_tensor = tf.convert_to_tensor(dataset['decoder_input_ids'])  # Convert decoder input ids\n","    decoder_attention_mask_tensor = tf.convert_to_tensor(dataset['decoder_attention_mask'])  # Convert decoder attention mask\n","    image_input_tensor = tf.convert_to_tensor(dataset['image_input'])  # Convert image data\n","\n","    return tf.data.Dataset.from_tensor_slices(({\n","                'input_ids': input_ids_tensor,  # Inputs\n","                'attention_mask': attention_mask_tensor,  # Attention mask for inputs\n","                'decoder_input_ids': decoder_input_ids_tensor,  # Decoder inputs\n","                'decoder_attention_mask': decoder_attention_mask_tensor,  # Attention mask for decoder\n","                'image_input': image_input_tensor  # Image inputs\n","            },\n","            decoder_input_ids_tensor  # Targets, shifted by one\n","           )).shuffle(1000).batch(16)\n","\n","train_dataset = prepare_dataset(processed_dataset)\n"]},{"cell_type":"code","execution_count":null,"id":"MKFyWZT0J3B4","metadata":{"id":"MKFyWZT0J3B4"},"outputs":[],"source":["for batch in train_dataset.take(1):\n","    print(batch[0]['input_ids'].shape)\n","    print(batch[0]['attention_mask'].shape)\n","    print(batch[0]['decoder_input_ids'].shape)\n","    print(batch[0]['decoder_attention_mask'].shape)\n","    print(batch[0]['image_input'].shape)\n","    print(batch[1].shape)"]},{"cell_type":"markdown","id":"037ad714","metadata":{"id":"037ad714"},"source":["## Step 4: Train the Model\n","Finally, we train the model using the prepared dataset."]},{"cell_type":"code","execution_count":null,"id":"Uebn9jc1COxH","metadata":{"id":"Uebn9jc1COxH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"6b4c492d","metadata":{"id":"6b4c492d"},"outputs":[],"source":["# Finally, train the multimodal model by passing the prepared dataset\n","# We train for 5 epochs (iterations over the dataset)\n","\n","# Train the model\n","history = multimodal_model.fit(train_dataset, epochs=5)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}