{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/nwm-llm-program/blob/main/Week1/Week1_Homework_Sentiment_Analysis_with_2_Layer_Transformer_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1320b7e",
      "metadata": {
        "id": "d1320b7e"
      },
      "source": [
        "\n",
        "# Sentiment Analysis with a Simple 2-Layer Transformer Encoder\n",
        "\n",
        "In this exercise, you'll learn how to create and train a simple 2-layer transformer encoder to perform sentiment analysis.\n",
        "We'll be using the IMDB dataset, which contains movie reviews labeled as positive or negative.\n",
        "\n",
        "This exercise should take approximately 30 minutes to complete.\n",
        "\n",
        "## Objectives\n",
        "- Prepare the IMDB dataset for sentiment analysis using the `TextVectorization` layer.\n",
        "- Build a simple 2-layer transformer encoder model.\n",
        "- Train the model on the IMDB dataset.\n",
        "- Evaluate the model's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae17375",
      "metadata": {
        "id": "eae17375"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Dense, LayerNormalization, MultiHeadAttention, Dropout, Layer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(train_data, test_data), info = tfds.load('imdb_reviews', split=['train', 'test'], with_info=True, as_supervised=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10fe52b9",
      "metadata": {
        "id": "10fe52b9"
      },
      "source": [
        "\n",
        "## Data Preparation\n",
        "\n",
        "We'll use the `TextVectorization` layer to preprocess the text data, converting it into integer sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edf75ba",
      "metadata": {
        "id": "1edf75ba"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define constants\n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "# Create the TextVectorization layer\n",
        "vectorize_layer = None  # TODO: Create the TextVectorization layer\n",
        "\n",
        "# Adapt the layer to the training data\n",
        "train_text = train_data.map(lambda x, y: x)\n",
        "# Adapt the vectorize_layer\n",
        "None\n",
        "\n",
        "# Prepare the datasets\n",
        "def vectorize_text(text, label):\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "train_data = train_data.map(vectorize_text).cache().shuffle(10000).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_data = test_data.map(vectorize_text).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in train_data.take(5):\n",
        "    print(text_batch.shape, label_batch.shape)"
      ],
      "metadata": {
        "id": "QkhCWRmElkrR"
      },
      "id": "QkhCWRmElkrR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6be277cc",
      "metadata": {
        "id": "6be277cc"
      },
      "source": [
        "\n",
        "## Model Construction\n",
        "\n",
        "We'll build a simple 2-layer transformer encoder. The model will consist of an embedding layer, two transformer encoder layers, and a final dense layer for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe246d38",
      "metadata": {
        "id": "fe246d38"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = None  # TODO: Create the MultiHeadAttention layer\n",
        "        self.ffn = None # Create the FFN layer with ff_dim first and then with embed_dim units.\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = None  # TODO: Pass the inputs through the att layer\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = None  # TODO: Pass the out1 through the ffn layer\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Build the model\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "x = Embedding(max_features, embed_dim)(inputs)\n",
        "x = TransformerEncoderLayer(embed_dim, num_heads, ff_dim)(x, training=True)\n",
        "x = TransformerEncoderLayer(embed_dim, num_heads, ff_dim)(x, training=True)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss=BinaryCrossentropy(), metrics=[BinaryAccuracy()])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd1de9a1",
      "metadata": {
        "id": "bd1de9a1"
      },
      "source": [
        "\n",
        "## Model Training\n",
        "\n",
        "We'll train the model using the IMDB dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04177b5",
      "metadata": {
        "id": "a04177b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train the model\n",
        "history = None  # TODO: Train the model on the train_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4445090",
      "metadata": {
        "id": "d4445090"
      },
      "source": [
        "\n",
        "## Model Evaluation\n",
        "\n",
        "Finally, we'll evaluate the model's performance on the test set to see how well it has learned to classify sentiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c9612a2",
      "metadata": {
        "id": "7c9612a2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_data)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hi5eE2WSletc"
      },
      "id": "Hi5eE2WSletc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}